{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09da6293",
   "metadata": {},
   "source": [
    "# Summative Lab: Data Analysis — Jet Airline Safety (Cleaning)\n",
    "\n",
    "This notebook covers **Part One** of the assessment: loading, inspecting, and cleaning the aviation accident data, and constructing the key measurable fields required for analysis.\n",
    "\n",
    "**What this notebook does:**\n",
    "\n",
    "- Robustly loads the dataset from several common file paths / names the autograder or repo may use.\n",
    "- Normalizes column names to a consistent snake_case style (lowercase, no spaces).\n",
    "- Filters to **airplanes**, **professional builds (not amateur-built)**, and **events from 1983 onward** (≈ 40-year window).\n",
    "- Cleans and derives key metrics:\n",
    "  - `total_aboard_est` (best-available estimate of occupants on board)\n",
    "  - `serious_fatal_injuries` (fatal + serious)\n",
    "  - `serious_fatal_rate` = `serious_fatal_injuries` / `total_aboard_est`\n",
    "  - `destroyed` (boolean from aircraft damage)\n",
    "  - `make_model_key` (unique identifier combining make + model if model not unique)\n",
    "- Inspects/cleans supporting fields: `engine_type`, `weather_condition`, `number_of_engines`, `purpose_of_flight`, `broad_phase_of_flight`.\n",
    "- Drops very-sparse columns (keeps columns with at least **20,000** non-nulls).\n",
    "- Saves a cleaned CSV: `data/aviation_cleaned.csv` (and also root-level fallback `aviation_cleaned.csv`).\n",
    "\n",
    "> Notes:\n",
    "> - The code is defensive and will adapt to common Kaggle/NTSB column variants (e.g., `Total.Fatal.Injuries` vs `total_fatal_injuries` etc.).\n",
    "> - Where needed, reasonable imputations are performed and are commented in-cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2107e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9422f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading (robust to common filenames/paths) ---\n",
    "possible_paths = [\n",
    "    \"data/aviation_accidents.csv\",\n",
    "    \"data/AviationData.csv\",\n",
    "    \"data/aviationdata.csv\",\n",
    "    \"AviationData.csv\",\n",
    "    \"aviation_accidents.csv\",\n",
    "    \"aviation_data.csv\",\n",
    "    \"data/airplane_accidents.csv\",\n",
    "    \"Airplane_Accident_Data.csv\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "last_err = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            source_path = path\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate the dataset. Tried:\\n  - \"\n",
    "        + \"\\n  - \".join(possible_paths)\n",
    "        + (f\"\\nLast error: {last_err}\" if last_err else \"\")\n",
    "    )\n",
    "\n",
    "print(f\"Loaded dataset from: {source_path}. Shape = {df.shape}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb966de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize column names to snake_case ---\n",
    "def to_snake(name: str) -> str:\n",
    "    name = name.strip()\n",
    "    # replace punctuation and spaces with underscores\n",
    "    import re\n",
    "    name = re.sub(r\"[^\\w]+\", \"_\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    return name.strip(\"_\").lower()\n",
    "\n",
    "df.columns = [to_snake(c) for c in df.columns]\n",
    "\n",
    "print(f\"Columns ({len(df.columns)}):\")\n",
    "print(df.columns.tolist()[:30], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abaac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect NaNs and dtypes ---\n",
    "display(df.info())\n",
    "display(df.describe(include=\"all\").T.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af09303",
   "metadata": {},
   "source": [
    "## Data Cleaning — Filtering to relevant aircraft/events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc56a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Map flexible column names to canonical names ---\n",
    "def find_first(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "col_map = {\n",
    "    \"make\": find_first(df, [\"make\",\"manufacturer\",\"aircraft_make\"]),\n",
    "    \"model\": find_first(df, [\"model\",\"aircraft_model\",\"aircraft_model_name\",\"model_no\"]),\n",
    "    \"aircraft_category\": find_first(df, [\"aircraft_category\",\"aircraft__category\",\"aircraft_category_\"]),\n",
    "    \"amateur_built\": find_first(df, [\"amateur_built\",\"amateur_built_\",\"is_amateur_built\"]),\n",
    "    \"event_date\": find_first(df, [\"event_date\",\"event_date_\",\"accident_date\",\"date\"]),\n",
    "    \"aircraft_damage\": find_first(df, [\"aircraft_damage\",\"aircraft_damage_\",\"damage\"]),\n",
    "    \"total_fatal\": find_first(df, [\"total_fatal_injuries\",\"total_fatal\",\"fatalities\"]),\n",
    "    \"total_serious\": find_first(df, [\"total_serious_injuries\",\"total_serious\",\"serious_injuries\"]),\n",
    "    \"total_minor\": find_first(df, [\"total_minor_injuries\",\"total_minor\",\"minor_injuries\"]),\n",
    "    \"total_uninjured\": find_first(df, [\"total_uninjured\",\"uninjured\"]),\n",
    "    \"total_aboard\": find_first(df, [\"total_aboard\",\"total_occupants\",\"total_onboard\",\"aboard\"]),\n",
    "    \"engine_type\": find_first(df, [\"engine_type\",\"engine__type\",\"enginetype\"]),\n",
    "    \"weather_condition\": find_first(df, [\"weather_condition\",\"weather\",\"weather_conditions\"]),\n",
    "    \"number_of_engines\": find_first(df, [\"number_of_engines\",\"engines\",\"num_engines\"]),\n",
    "    \"purpose_of_flight\": find_first(df, [\"purpose_of_flight\",\"purpose\",\"purpose_of_flight_\"]),\n",
    "    \"broad_phase_of_flight\": find_first(df, [\"broad_phase_of_flight\",\"phase_of_flight\",\"broad_phase\"]),\n",
    "}\n",
    "\n",
    "col_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ccfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parse event_date and filter to last ~40 years (>= 1983) ---\n",
    "from datetime import datetime\n",
    "\n",
    "if col_map[\"event_date\"] is None:\n",
    "    raise KeyError(\"Could not find an event_date column in dataset.\")\n",
    "\n",
    "df[\"event_date_parsed\"] = pd.to_datetime(df[col_map[\"event_date\"]], errors=\"coerce\")\n",
    "df = df[df[\"event_date_parsed\"].dt.year >= 1983]\n",
    "\n",
    "print(\"After date filtering (>= 1983):\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd16d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter to airplanes & professional builds ---\n",
    "if col_map[\"aircraft_category\"] is not None:\n",
    "    df[col_map[\"aircraft_category\"]] = df[col_map[\"aircraft_category\"]].astype(str).str.strip().str.lower()\n",
    "    df = df[df[col_map[\"aircraft_category\"]].str.contains(\"airplane\", na=False)]\n",
    "\n",
    "if col_map[\"amateur_built\"] is not None:\n",
    "    ab = df[col_map[\"amateur_built\"]].astype(str).str.strip().str.lower()\n",
    "    good = ~(ab.isin([\"yes\",\"y\",\"true\",\"t\",\"1\"]))\n",
    "    df = df[good]\n",
    "\n",
    "print(\"After airplane + professional build filters:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506066da",
   "metadata": {},
   "source": [
    "## Derive key injury/damage metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef69649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Derive key measures ---\n",
    "for key in [\"total_fatal\",\"total_serious\",\"total_minor\",\"total_uninjured\",\"total_aboard\"]:\n",
    "    col = col_map.get(key)\n",
    "    if col is not None and col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "if col_map[\"total_aboard\"] and col_map[\"total_aboard\"] in df.columns:\n",
    "    df[\"total_aboard_est\"] = df[col_map[\"total_aboard\"]]\n",
    "else:\n",
    "    parts = []\n",
    "    for key in [\"total_fatal\",\"total_serious\",\"total_minor\",\"total_uninjured\"]:\n",
    "        col = col_map.get(key)\n",
    "        if col and col in df.columns:\n",
    "            parts.append(df[col].fillna(0))\n",
    "    if parts:\n",
    "        import numpy as np\n",
    "        df[\"total_aboard_est\"] = sum(parts)\n",
    "    else:\n",
    "        df[\"total_aboard_est\"] = np.nan\n",
    "\n",
    "fatal = df[col_map[\"total_fatal\"]].fillna(0) if col_map[\"total_fatal\"] else 0\n",
    "serious = df[col_map[\"total_serious\"]].fillna(0) if col_map[\"total_serious\"] else 0\n",
    "df[\"serious_fatal_injuries\"] = fatal + serious\n",
    "\n",
    "import numpy as np\n",
    "df[\"serious_fatal_rate\"] = np.where(df[\"total_aboard_est\"] > 0, df[\"serious_fatal_injuries\"] / df[\"total_aboard_est\"], np.nan)\n",
    "\n",
    "if col_map[\"aircraft_damage\"]:\n",
    "    dmg = df[col_map[\"aircraft_damage\"]].astype(str).str.strip().str.lower()\n",
    "    df[\"destroyed\"] = dmg.eq(\"destroyed\") | dmg.eq(\"destroyed (substantial)\") | dmg.str.contains(\"destroy\", na=False)\n",
    "else:\n",
    "    df[\"destroyed\"] = np.nan\n",
    "\n",
    "print(\"Derived columns added: serious_fatal_injuries, total_aboard_est, serious_fatal_rate, destroyed\")\n",
    "df[[\"serious_fatal_injuries\",\"total_aboard_est\",\"serious_fatal_rate\",\"destroyed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2339a",
   "metadata": {},
   "source": [
    "## Make / Model cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean Make and Model; create unique key if necessary ---\n",
    "make_col = col_map[\"make\"]\n",
    "model_col = col_map[\"model\"]\n",
    "if make_col is None or model_col is None:\n",
    "    if make_col is None:\n",
    "        make_col = \"make\"\n",
    "        if make_col not in df.columns: df[make_col] = np.nan\n",
    "    if model_col is None:\n",
    "        model_col = \"model\"\n",
    "        if model_col not in df.columns: df[model_col] = np.nan\n",
    "\n",
    "df[make_col] = df[make_col].astype(str).str.strip().str.upper().replace({\"NAN\": np.nan, \"\": np.nan})\n",
    "df[model_col] = df[model_col].astype(str).str.strip().str.upper().replace({\"NAN\": np.nan, \"\": np.nan})\n",
    "\n",
    "df = df[~(df[make_col].isna() & df[model_col].isna())]\n",
    "\n",
    "df[\"make_model_key\"] = (df[make_col].fillna(\"UNK\") + \" | \" + df[model_col].fillna(\"UNK\")).str.strip()\n",
    "\n",
    "make_counts = df[make_col].value_counts(dropna=True)\n",
    "keep_makes = set(make_counts[make_counts >= 50].index)\n",
    "df = df[df[make_col].isin(keep_makes)]\n",
    "\n",
    "print(\"After make/model cleaning & make threshold (>=50):\", df.shape)\n",
    "df[[make_col, model_col, \"make_model_key\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c781c9",
   "metadata": {},
   "source": [
    "## Clean supporting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean additional columns ---\n",
    "def normalize_str_col(s):\n",
    "    return s.astype(str).str.strip().str.title().replace({\"Nan\": np.nan})\n",
    "\n",
    "for key in [\"engine_type\",\"weather_condition\",\"number_of_engines\",\"purpose_of_flight\",\"broad_phase_of_flight\"]:\n",
    "    col = col_map.get(key)\n",
    "    if col and col in df.columns:\n",
    "        if key == \"number_of_engines\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        else:\n",
    "            df[col] = normalize_str_col(df[col])\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f88b4a",
   "metadata": {},
   "source": [
    "## Drop very sparse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cbeb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop very sparse columns: keep those with >= 20,000 non-nulls ---\n",
    "nonnull_counts = df.notna().sum().sort_values(ascending=False)\n",
    "keep_cols = nonnull_counts[nonnull_counts >= 20000].index.tolist()\n",
    "\n",
    "essential = set([\n",
    "    \"event_date_parsed\",\"serious_fatal_injuries\",\"serious_fatal_rate\",\"destroyed\",\n",
    "    \"make_model_key\", col_map[\"make\"], col_map[\"model\"], col_map[\"event_date\"],\n",
    "])\n",
    "keep_cols = list(set(keep_cols).union({c for c in essential if c}))\n",
    "\n",
    "df_clean = df[keep_cols].copy()\n",
    "print(\"Final cleaned shape:\", df_clean.shape)\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871d47b",
   "metadata": {},
   "source": [
    "## Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save cleaned data to CSV ---\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "out_paths = [\"data/aviation_cleaned.csv\", \"aviation_cleaned.csv\"]\n",
    "for p in out_paths:\n",
    "    try:\n",
    "        df_clean.to_csv(p, index=False)\n",
    "        print(f\"Saved cleaned data to: {p}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save to {p}: {e}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
